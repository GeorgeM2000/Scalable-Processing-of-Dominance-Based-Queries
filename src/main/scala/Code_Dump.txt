/*
    // Set up Spark configuration
    val conf = new SparkConf().setAppName("Skyline").setMaster("local")
    val sc = new SparkContext(conf)

    // Example usage:
    val points = sc.parallelize(Seq(
      Point(Array(1.0, 1.0)),
      Point(Array(2.0, 2.0)),
      Point(Array(3.0, 3.0)),
      Point(Array(0.5, 2.0)),
      Point(Array(4.0, 4.0)),
      Point(Array(5.0, 5.0)),
      Point(Array(6.0, 6.0)),
      Point(Array(0.2, 1.3)),
      Point(Array(8.0, 8.0)),
      Point(Array(9.0, 9.0))
    ))


    // Compute local skylines
    val localSkylines = points.mapPartitions(Skyline_Calculation.computeLocalSkyline).collect()

    // Compute global skylines
    val globalSkylines = Skyline_Calculation.computeGlobalSkyline(localSkylines.iterator)

    // Broadcast the Global Skyline Set to workers
    val globalSkylinesBroadcast = sc.broadcast(globalSkylines.toList)

    val results = points.mapPartitions { iter =>

      val localDominatedPoints = Skyline_Calculation.loadDominatedPointsFromLocalFileSystem()

      // Capture the global skyline iterator in the closure
      val globalSkylineIteratorOnWorker = globalSkylinesBroadcast.value

      for(s <- globalSkylineIteratorOnWorker) {
        for(d <- localDominatedPoints) {
          if(Skyline_Calculation.dominates(s, d)) {
            s.dominance_score += 1
          }
        }
      }

      globalSkylineIteratorOnWorker.iterator
    }


    results.foreach(println)

    // Print the top-k skyline points with the highest dominance score
    //val topK = 5 // Change this value to the desired top-k
    //val topKSkyline = globalDominanceScores.take(topK)

    //println(s"\nTop-$topK Skyline Points with Highest Dominance Score:")
    //topKSkyline.foreach(println)

    // Stop the SparkContext
    sc.stop()

     */

    /*
    val D = Iterator(
      Point(List(1.0, 1.0)),
      Point(List(2.0, 2.0)),
      Point(List(3.0, 3.0)),
      Point(List(0.5, 2.0)),
      Point(List(4.0, 4.0)),
      Point(List(5.0, 5.0)),
      Point(List(6.0, 6.0)),
      Point(List(0.2, 1.3)),
      Point(List(8.0, 8.0)),
      Point(List(9.0, 9.0))
    )

    val skylineSet = Skyline_Calculation.computeSkyline(D)
    println("Skyline Set:")
    skylineSet.foreach(println)

     */


         /*
             val D = Iterator(
               Point(Array(1.0, 1.0)),
               Point(Array(2.0, 2.0)),
               Point(Array(3.0, 3.0)),
               Point(Array(0.5, 2.0)),
               Point(Array(4.0, 4.0)),
               Point(Array(5.0, 5.0)),
               Point(Array(6.0, 6.0)),
               Point(Array(0.2, 0.9)),
               Point(Array(8.0, 8.0)),
               Point(Array(9.0, 9.0))
               //Point(Array(0.1, 0.1))
             )

              */

         val D = Iterator(
           Point(Array(1.0, 2.0)),
           Point(Array(1.5, 1.5)),
           Point(Array(2.5, 1.0)),
           Point(Array(1.25, 2.2)),
           Point(Array(1.25, 2.3)),
           Point(Array(1.6, 1.6)),
           Point(Array(1.6, 1.8)),
           Point(Array(2.6, 1.2)),
           Point(Array(2.15, 2.15)),
           Point(Array(2.20, 2.15)),
           Point(Array(2.10,2.40)),
           Point(Array(2.12,2.12)),
           Point(Array(3.15,2.25)),
           Point(Array(3.20,2.30))
         )

         val skylineSet = Skyline_Calculation.calculate(D)
         println("Skyline Set:")
         skylineSet.foreach(println)



  import scala.math._

  def createVariablePartitions(lines: Int, numberOfPartitions: Int): Vector[Vector[Int]] = {
    val partitionSize: Int = floor(lines.toDouble / numberOfPartitions).toInt
    var partitions: Vector[Vector[Int]] = Vector()

    // Add variables 0 - N-1 to partitions
    var lastIndex = 0
    for (i <- 1 until numberOfPartitions) {
      var partition: Vector[Int] = Vector()

      var j: Int = partitionSize * (i - 1)
      partition :+= j + 1
      partition :+= j + partitionSize

      partitions :+= partition
      lastIndex = i
    }

    partitions :+= Vector[Int](partitionSize * lastIndex + 1, (((partitionSize * lastIndex) + (partitionSize - 1)) + (lines - (partitionSize * numberOfPartitions))) + 1)

    partitions
  }


//val txtFile = sc.wholeTextFiles(s"/home/georgematlis/IdeaProjects/Scalable Processing of Dominance-Based Queries/Input/*")


/*
    val localSkylines = txtFile.map { case (_, content) =>
      val partition = content.split("\n").map(_.split(",").map(_.toDouble))
      println(partition)
      println("========")
      val p1 = partition.map(array => Point(array)).iterator
      // content.split("\n").map(_.split(",").map(_.toDouble)).map(array => Point(array)).iterator

      Skyline_Calculation.computeLocalSkyline(p1)
    }
     */

    /*
    // Assuming localSkylines: Iterator[Iterator[Point]]
    for (partitionIterator <- localSkylines) {
      for (point <- partitionIterator) {
        // Process each Point in the local skyline
        println(point)
      }
    }

     */




    //val r = localSkylines.mapPartitions(Skyline_Calculation.computeLocalSkyline)

    /*
    val localSkylines = txtFile.flatMap { case (filePath, content) =>
      content.split("\n").map(_.split(",")).map { line =>
        val coordinates = line.map(_.toDouble)
        Point(coordinates)
      }
    }.mapPartitions(Skyline_Calculation.computeLocalSkyline)

     */



def hashMapping(dPoint: Array[Double]): Int = {
    // Convert the d-dimensional point to a string for hashing
    val pointStr = dPoint.mkString(",")

    // Calculate the hash value
    val originalHash = MurmurHash3.stringHash(pointStr)

    originalHash.toInt
  }

def saveDominatedPointsToLocalFileSystem(dominatedPoints: ListBuffer[Point]): Unit = {

    // Assuming each partition writes to a separate file
    val partitionId = TaskContext.getPartitionId()
    val outputFilePath = new File(s"/home/georgematlis/IdeaProjects/Scalable Processing of Dominance-Based Queries/Input_Partitions/partition_$partitionId.txt")

    // Open a new PrintWriter for writing Dominated_Points
    val writer = new PrintWriter(outputFilePath)

    // Iterate over the ListBuffer and write each point to the file
    dominatedPoints.foreach { point =>
      // Convert point to a string representation as needed
      writer.println(point.dimensionValues.mkString(","))
    }

    // Close the writer
    writer.close()
  }

  def loadDominatedPointsFromLocalFileSystem(): List[Point] = {
    // Create an empty ListBuffer to store Dominated_Points
    val dominatedPoints = ListBuffer[Point]()

    // Assuming each partition wrote to a separate file
    val partitionId = TaskContext.getPartitionId()
    val inputFilePath = new File(s"/home/georgematlis/IdeaProjects/Scalable Processing of Dominance-Based Queries/Input_Partitions/partition_$partitionId.txt")

    // Read Dominated_Points from the file and add them to the ListBuffer
    val reader = new BufferedReader(new FileReader(inputFilePath))
    var line: String = null


    while ( {
      line = reader.readLine(); line != null
    }) {
      val point: Point = {
        // Parse the line and split dimension values
        val values = line.split(",").map(_.trim.toDouble)

        // Create a Point object using the parsed dimension values
        Point(dimensionValues = values)
      }

      dominatedPoints += point
    }

    // Close the reader
    reader.close()

    // Return the ListBuffer containing Dominated_Points
    dominatedPoints.toList
  }

// Function to compute dominance scores locally on each worker
  def computeLocalDominanceScores(iterator: Iterator[Point]): Iterator[Point] = {
    val points = iterator.toArray

    for {
      p1 <- points
      p2 <- points if p1 != p2
    } {
      if(dominates(p1, p2)) {
        p1.dominance_score += 1
      }
    }

    points.iterator
  }

// Assuming results is an RDD[Point]
    val combinedResults = results
      .map(point => (point, point.dominance_score)) // Convert Point to (Point, dominance_score) pair
      .aggregateByKey(0)(_ + _, _ + _) // Aggregate dominance scores by key (Point)

    // Extract the top dominating skyline points
    val topDominatingGlobalSkylinePoints = combinedResults
      .sortBy({ case (_, totalDominance) => totalDominance }, ascending = false)
      .map({ case (point, _) => point })

    //val topDominatingGlobalSkylinePoints = results.collect().distinct().sortBy(point => point.dominance_score, ascending = false)

    topDominatingGlobalSkylinePoints.foreach(println)



/*

    val start = System.nanoTime()
    val D = Iterator(
      Point(Array(1.0, 1.0)),
      Point(Array(2.0, 2.0)),
      Point(Array(3.0, 3.0)),
      Point(Array(0.5, 2.0)),
      Point(Array(4.0, 4.0)),
      Point(Array(5.0, 5.0)),
      Point(Array(6.0, 6.0)),
      Point(Array(0.2, 0.9)),
      Point(Array(8.0, 8.0)),
      Point(Array(9.0, 9.0))
      //Point(Array(0.1, 0.1))
    )

    val skylineSet = Skyline_Calculation.computeFastSkyline(D)

    println((System.nanoTime() - start).asInstanceOf[Double] / 1000000000.0)
    println("Skyline Set:")
    skylineSet.foreach(println)

     */
     
     
     
=====================================================

object Main {
  case class Point(id: Int, values: List[Double], sum: Double)

  def readCsv(filename: String): RDD[Point] = {
    val records = new ListBuffer[Point]()
    val source = Source.fromFile(filename)
    var l = 0
    for (line <- source.getLines()) {
      if (l != 0) {
        val d = line.trim.split(",") //split with ,
        val values = d.tail.map(_.toDouble).toList // except for the first element the others refer to values
        records += Point(d.head.toInt, values, values.sum) //create a record
      }
      l += 1
    }
    source.close()

    SparkSession.builder().getOrCreate()
      .sparkContext.parallelize(
      records.toList.sortWith((x, y) => x.sum > y.sum)) //sort based on sum revert
  }

  def i_dominates_j(a: Point, b: Point): Boolean = {
    a.values
      .zip(b.values) //combine the 2 values
      .count(x => x._1 > x._2) == 0 // count how many values of a are greater than b's
  }

  def skyline(dataPoints:RDD[Point]):RDD[Int]={
    dataPoints.cartesian(dataPoints) //create the cartesian product of data points with itself
      .filter(x => x._1.id != x._2.id) //remove the pairs that refer to the same id
      .map(x=>{
        (x._2.id, if(i_dominates_j(x._1,x._2)) 1 else 0)
      }) //0 if _2 is not getting dominated, 1 if it does
      .reduceByKey(_+_)
      .filter(_._2>0) //keep the nodes that are not dominated by any other node
      .map(_._1)
  }


  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf()
      .setMaster("local[1]") //The number of threads to use.
      .setAppName("Skyline")
    val spark = SparkSession.builder().config(sparkConf).getOrCreate()
    val sc = spark.sparkContext
    Logger.getLogger("org").setLevel(Level.OFF)


    var startTime = System.currentTimeMillis()
    val csvName_list: List[String] = List("datasets/norm_dimPoints_20_numPoints_10000.csv")
    for (csvName <- csvName_list) {
      println(csvName)
      // --- Task 1 --- //
      val dataPoints_list:RDD[Point] = readCsv(csvName)
      val skyline_list = skyline(dataPoints_list).collect()
      println("Skyline: ")
      skyline_list.foreach(x=> println(s"Point $x"))

      var finishTime = System.currentTimeMillis()
      print(finishTime - startTime)
      println("ms")
//       --- Task 2 --- //
      startTime = System.currentTimeMillis()
      val k = 10
      val dominatedPoints_sorted_list:RDD[(Point,Int)] = dataPoints_list
        .cartesian(dataPoints_list) //create the cartesian product of data points with itself
        .filter(x => x._1.id != x._2.id) //remove the pairs that refer to the same id
        .filter(a => i_dominates_j(a._1, a._2)) //keep only the pairs that the first one dominates the second one
        .map(x => (x._1, 1))
        .reduceByKey(_+_)
        .sortBy(_._2,ascending = false)

        dominatedPoints_sorted_list.persist(StorageLevel.MEMORY_AND_DISK)


      println(s"The top-$k points with the highest dominance score: ")
        dominatedPoints_sorted_list.take(k).foreach(x=>println(s"${x._1.id} with score ${x._2}")) //print them

      finishTime = System.currentTimeMillis()
      print(finishTime - startTime)
      println("ms")

      // --- Task 3 --- //
      startTime = System.currentTimeMillis()
      println(s"The top-$k points in the skyline with the highest dominance score: ")
      val bc = sc.broadcast(skyline_list)
      dominatedPoints_sorted_list
        .filter(x=>bc.value.contains(x._1.id)) //maintain only the points that belong to the skyline
        .take(k) //keep only the top k
        .foreach(x=>println(s"Point ${x._1.id} with score ${x._2}"))

      finishTime = System.currentTimeMillis()
      print(finishTime - startTime)
      println("ms")
    }
  }
}

=============================================================
def dominatesM(p1: Point, p2: Point): Boolean = {
    var isDominating = false
    var isAtLeastEqual = false

    for (i <- p1.dimensionValues.indices) {
      if (p1.dimensionValues(i) < p2.dimensionValues(i)) {
        isDominating = true
      } else if (p1.dimensionValues(i) > p2.dimensionValues(i)) {
        return false
      }
      else {
        isAtLeastEqual = true
      }
    }

    isDominating || isAtLeastEqual
  }


/*
  // This is a faster version of the baseline skyline algorithm
  def computeFastSkyline(Data: Iterator[Point]): Iterator[Point] = {
    var arraybuffer = ArrayBuffer[Point]()
    val array = Data.toArray
    arraybuffer += array(0)
    for (i<-1 until array.length)
    {
      var j=0
      var breaked = false
      breakable
      {
        while (j < arraybuffer.length) {
          if (dominates(array(i), arraybuffer(j))) {
            arraybuffer.remove(j)
            j-=1
          }
          else if (dominates(arraybuffer(j), array(i))) {
            breaked = true
            break()
          }
          j += 1
        }
      }
      if(!breaked)
        arraybuffer+=array(i)
    }
    arraybuffer.iterator
  }
   */







// Compute the local skyline set S
  // This is an alternative version of the baseline skyline algorithm. It doesn't compute the dominance score
  def computeLocalSkylineAltBaseline(Data: Iterator[Point]): Iterator[Point] = {
    val S = ListBuffer[Point]()
    //val Dominated_Points = ListBuffer[Point]()
    var Points = Data

    while (Points.hasNext) {
      val p1 = Points.next()
      var dominated = false

      val iteratorClone = Points.filter { p2 =>
        if (dominates(p1, p2)) {
          /*
          if(!Dominated_Points.contains(p2)) {
            Dominated_Points += p2
          }
           */
          dominated = true
          false
        } else if (dominates(p2, p1)) {
          /*
          if (!Dominated_Points.contains(p1)) {
            Dominated_Points += p1
          }
           */
          S -= p1
          true
        } else {
          true
        }
      }

      if (!dominated) {
        S += p1
        Points = iteratorClone
      }
    }

    S.iterator
  }


  /*
  // Compute the local skyline set S
  // This is not the baseline skyline algorithm. It computes the dominance score for each skyline point
  def computeGlobalSkyline(Data: Iterator[Point]): Iterator[Point] = {

    val S = ListBuffer[Point]()
    val Dominated_Points = ListBuffer[Point]()
    var Points = Data

    while (Points.hasNext) {
      val p1 = Points.next()
      var dominated = false

      val iteratorClone = Points.filter { p2 =>
        if (dominates(p1, p2)) {
          if (!Dominated_Points.contains(p2)) {
            Dominated_Points += p2
          }
          dominated = true
          false
        } else if (dominates(p2, p1)) {
          if (!Dominated_Points.contains(p1)) {
            Dominated_Points += p1
          }
          S -= p1
          true
        } else {
          true
        }
      }

      if (!dominated) {
        S += p1
        Points = iteratorClone
      }
    }

    for(s <- S.toList) {
      for(d <- Dominated_Points.toList) {
        if(dominates(s, d)) {
          s.dominance_score += 1
        }
      }
    }

    S.iterator
  }
   */

def dominates(p1: Point, p2: Point): Boolean = {
    p1.dimensionValues.zip(p2.dimensionValues).forall { case (x, y) => x <= y } && p1.dimensionValues.exists(_ < p2.dimensionValues.min)
  }



====================================================================

//val start = System.nanoTime()

    //val txtFile = sc.textFile(inputFile, minPartitions = numberOfPartitions)


    /*
    // Task 2
    val dominanceScoreCalcForPoints:RDD[(Point, Int)] = txtFile.map(line => line.split(","))
      .map(line => line.map(elem => elem.toDouble))
      .map(array => Point(array))
      .cartesian(dominanceScoreCalcForPoints)
      .filter { case (point1, point2) =>
      point1 != point2 && dominates(point1, point2)
    }.map(x => (x._1, 1))
      .reduceByKey(_+_)
      .sortBy(_._2,ascending = false)

     */

    /*
    val localSkylines = txtFile.map(line => line.split(","))
      .map(line => line.map(elem => elem.toDouble))
      .map(array => Point(array))
      .mapPartitions(Skyline_Calculation.computeLocalSkylineBaseline)

    //println((System.nanoTime() - start).asInstanceOf[Double] / 1000000000.0)

    val globalSkylines = Skyline_Calculation.computeLocalSkylineBaseline(localSkylines.collect().iterator)

    println((System.nanoTime() - start).asInstanceOf[Double] / 1000000000.0)

     */

    //globalSkylines.foreach(println)



    /*
    // Local skyline computation using the euclidean distance as a scoring function to sort the d-dimensional points
    val localSkylines = txtFile.map(line => line.split(","))
      .map(line => line.map(elem => elem.toDouble))
      .map(array => Point(array, distance_score = euclideanDistance(array)))
      .sortBy(point => point.distance_score)
      .mapPartitions(Skyline_Calculation.computeFastSkyline)

     */

    //println((System.nanoTime() - start).asInstanceOf[Double] / 1000000000.0)


    /*
    val datasetPreprocessing = txtFile.map(line => line.split(","))
      .map(line => line.map(elem => elem.toDouble))
      .map(array => Point(array, distance_score = euclideanDistance(array)))
      .sortBy(point => point.distance_score)
      .persist(StorageLevel.MEMORY_AND_DISK)

    val globalSkylines = Skyline_Calculation.computeGlobalSkyline(datasetPreprocessing
      .mapPartitions(Skyline_Calculation.computeLocalSkylineBaseline).collect().iterator)


    // Broadcast the Global Skyline Set to workers
    val globalSkylinesBroadcast = sc.broadcast(globalSkylines.toList)

    val results = datasetPreprocessing.mapPartitions {Data: Iterator[Point] =>
      val points = Data.toList
      val localSkylineIndices = Skyline_Calculation.loadLocalSkylineIndices()

      // Capture the global skyline iterator in the closure
      val globalSkylineIteratorOnWorker = globalSkylinesBroadcast.value

      val localDominatedPoints = Skyline_Calculation.findLocalDominatedPoints(localSkylineIndices.localPartitionLength, localSkylineIndices.localSkylineIndices)

      for (s <- globalSkylineIteratorOnWorker) {
        for (i <- localDominatedPoints) {
          if (Skyline_Calculation.dominates(s, points(i))) {
            s.dominance_score += 1
          }
        }
      }

      globalSkylineIteratorOnWorker.iterator
    }

    results.foreach(println)

     */
